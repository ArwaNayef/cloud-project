The final Project of Cloud course -Files Classifications 
#
For this project we had to deep dive not only into cloud but also into learning about data classification, sorting, and searching. This let us improve our skills and we could implement the methods and deliver good results. Here we’ll talk about each method and its approach:
<br>
1) Sort Method: <br>The approach we claimed to apply sorting on our files is getting the file title and then sort using those titles. We started with implementing the title Method which takes a file and returns its title from its metadata. After this we built the sorting method using the title method which takes folder path and sorts its files, it does so by creating a dictionary and looping the files to add each file with its title, after this we sort the files depending on the titles that are attached to them. We’ve used library from python that helped us such as PyPDF2 to reading the metadata for pdf files.<br>
2) Searching Method: This method takes two parameters: the text to be found and the path to the folder that contains the folders. It returns all documents that include the target text and highlights its position. It also provides information about the time taken to search, the total number of documents, and the number of documents that were returned as a result.
We first deleted all the highlighted documents in order to renew the highlights for each new search. Then, I opened all the files in all the existing folders and used the Search_for method to search for the required text. This method returns the position of the text and we store it in a variable. If the variable contained any values, this meant that the search had found matching words. we accessed these matches, added highlight annotations to them, and saved the modified files to a specific location.<br>
3) Two Methods for classifying documents:
> Build_tree: first we define two arrays: one for texts and the other for labels. Then we open all the files and store each file text in an index of array and its folder name in the labels array.Now we have two arrays with the same length.
Computer as a machine does not understand the texts clearly. So we have to convert all texts into an understandable numbers for machine.
So we used the countVectorizer ( Preprocessing step) which is a class in Python that converts a collection of text documents into a matrix of token counts by tokenizing the input text, building a vocabulary of unique tokens, and counting the occurrences of each token in each document. It has several parameters that control the tokenization and vocabulary building process. We use The stop words parameter (this parameter remove the words that are common in the language to minimize the matrix). The resulting matrix has one row for each document and one column for each token in the vocabulary, with the value in each cell being the number of times the corresponding token appears in the corresponding document.
Then we used TfidfTransformer ( Preprocessing step) which is a class in the sklearn.feature_extraction.text that converts a matrix of token counts into a matrix of term frequency-inverse document frequency (TF-IDF) weights. it applies a weighting scheme that measures the importance of each word in a document relative to a collection of documents. It takes a matrix of token counts as input, which is typically generated by a CountVectorizer object, and returns a matrix of TF-IDF weights with the same shape as the input matrix. It has several parameters that control the behavior of the weighting process.
After this we used a method to split data into training and testing to evaluate our classifier train_test_split()
Then we use RandomForestClassifier which is a class in the sklearn.ensemble module in Python that implements a random forest classifier, which is a type of ensemble learning algorithm. (Ensemble learning is a machine learning technique that combines the predictions of multiple models in order to improve the overall performance of the model). It takes a set of labeled training data as input and aggregates the predictions of the individual decision tree classifiers using a majority vote for classification tasks or an average for regression tasks. It has several parameters that control the number of trees in the forest, the size of the subsets, and the behavior of the decision tree classifiers.
After that, we applied this model to our files and built a RandomForest classifier then stored it for reference in the case of a new prediction and not creating it again.
##
>Predict method:
It just load the classifier and preprocesses the text as we mentioned in the building method. Then the cleaned text entered the loaded model and give the prediction label with a high accuracy.
For development we decided to use Python language as it can be a good choice for working with data. For python implementation, we used Jupyter IDE with Anaconda Prompt which can run locally and online. And for cloud we decided to go with Azure which is from Microsoft. We created an Ubuntu server and uploaded python files and folders to the cloud using it. It uploads the file into its predicted folder and it returns the time taken to predict.
